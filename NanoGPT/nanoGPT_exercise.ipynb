{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oIVmoSSP0Brw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 128 #B, how many independent sequences will we process in parallel\n",
        "block_size = 256 #T, what is the maximum context length for predictions\n",
        "n_embd = 128 #C, embedding length\n",
        "n_head = 4 # number of heads in transformer block\n",
        "#head_size = n_embd//n_head\n",
        "n_layer = 4 # how many transformer blocks\n",
        "\n",
        "max_iters = 5000 # how many batches in training\n",
        "eval_interval = 100 # how many batches between loss estimations in training\n",
        "eval_iters = 200 # how many batches for loss estimation\n",
        "\n",
        "learning_rate = 1e-3\n",
        "dropout = 0.3\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load Alice.txt to Colab\n",
        "with open('Alice.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "DcZTnPjk0avu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OXBlqbl0e3B",
        "outputId": "5422be0e-66cd-4fed-9a70-b308e47002d5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alice’s Adventures in Wonderland\n",
            "\n",
            "by Lewis Carroll\n",
            "\n",
            "THE MILLENNIUM FULCRUM EDITION 3.0\n",
            "\n",
            "Contents\n",
            "\n",
            " CHAPTER I.     Down the Rabbit-Hole\n",
            " CHAPTER II.    The Pool of Tears\n",
            " CHAPTER III.   A Caucus-Race and a Long Tale\n",
            " CHAPTER IV.    The Rabbit Sends in a Little Bill\n",
            " CHAPTER V.     Advice from a Caterpillar\n",
            " CHAPTER VI.    Pig and Pepper\n",
            " CHAPTER VII.   A Mad Tea-Party\n",
            " CHAPTER VIII.  The Queen’s Croquet-Ground\n",
            " CHAPTER IX.    The Mock Turtle’s Story\n",
            " CHAPTER X.     The Lobster Quadrille\n",
            " CHAPTER XI.    Who Stole the Tarts?\n",
            " CHAPTER XII.   Alice’s Evidence\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CHAPTER I.\n",
            "Down the Rabbit-Hole\n",
            "\n",
            "\n",
            "Alice was beginning to get very tired of sitting by her sister on the\n",
            "bank, and of having nothing to do: once or twice she had peeped into\n",
            "the book her sister was reading, but it had no pictures or\n",
            "conversations in it, “and what is the use of a book,” thought Alice\n",
            "“without pictures or conversations?”\n",
            "\n",
            "So she was considering in her own mind (as well as she could, for the\n",
            "hot day made her feel very \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#number of characters in the text\n",
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCAiJojQ0gpA",
        "outputId": "034c9f17-8d55-4103-b520-dc6e3eb88133"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "144584"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfbQrvWm0isL",
        "outputId": "f805642d-462a-42ab-a2c5-417c5a0f9ad8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '0',\n",
              " '3',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " '[',\n",
              " ']',\n",
              " '_',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " 'ù',\n",
              " '—',\n",
              " '‘',\n",
              " '’',\n",
              " '“',\n",
              " '”']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(chars)\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OifHc8M80kiM",
        "outputId": "b6482c54-b8c2-472d-fd07-2851749a190c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "75"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "#and from integers to characters\n",
        "itos = { i:ch for i,ch in enumerate(chars) }"
      ],
      "metadata": {
        "id": "SyewmzeT0mNs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stoi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8hjqkD10oon",
        "outputId": "c17c6332-852f-4f7e-9682-088f03394abe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '(': 3,\n",
              " ')': 4,\n",
              " '*': 5,\n",
              " ',': 6,\n",
              " '-': 7,\n",
              " '.': 8,\n",
              " '0': 9,\n",
              " '3': 10,\n",
              " ':': 11,\n",
              " ';': 12,\n",
              " '?': 13,\n",
              " 'A': 14,\n",
              " 'B': 15,\n",
              " 'C': 16,\n",
              " 'D': 17,\n",
              " 'E': 18,\n",
              " 'F': 19,\n",
              " 'G': 20,\n",
              " 'H': 21,\n",
              " 'I': 22,\n",
              " 'J': 23,\n",
              " 'K': 24,\n",
              " 'L': 25,\n",
              " 'M': 26,\n",
              " 'N': 27,\n",
              " 'O': 28,\n",
              " 'P': 29,\n",
              " 'Q': 30,\n",
              " 'R': 31,\n",
              " 'S': 32,\n",
              " 'T': 33,\n",
              " 'U': 34,\n",
              " 'V': 35,\n",
              " 'W': 36,\n",
              " 'X': 37,\n",
              " 'Y': 38,\n",
              " 'Z': 39,\n",
              " '[': 40,\n",
              " ']': 41,\n",
              " '_': 42,\n",
              " 'a': 43,\n",
              " 'b': 44,\n",
              " 'c': 45,\n",
              " 'd': 46,\n",
              " 'e': 47,\n",
              " 'f': 48,\n",
              " 'g': 49,\n",
              " 'h': 50,\n",
              " 'i': 51,\n",
              " 'j': 52,\n",
              " 'k': 53,\n",
              " 'l': 54,\n",
              " 'm': 55,\n",
              " 'n': 56,\n",
              " 'o': 57,\n",
              " 'p': 58,\n",
              " 'q': 59,\n",
              " 'r': 60,\n",
              " 's': 61,\n",
              " 't': 62,\n",
              " 'u': 63,\n",
              " 'v': 64,\n",
              " 'w': 65,\n",
              " 'x': 66,\n",
              " 'y': 67,\n",
              " 'z': 68,\n",
              " 'ù': 69,\n",
              " '—': 70,\n",
              " '‘': 71,\n",
              " '’': 72,\n",
              " '“': 73,\n",
              " '”': 74}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
      ],
      "metadata": {
        "id": "AfLVJCm80q7t"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:25])\n",
        "enc=encode(text[:25])\n",
        "print()\n",
        "print(enc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MwWldFi0tEh",
        "outputId": "bc81c392-fcc8-4d86-a4a7-890e6b602f79"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alice’s Adventures in Won\n",
            "\n",
            "[14, 54, 51, 45, 47, 72, 61, 1, 14, 46, 64, 47, 56, 62, 63, 60, 47, 61, 1, 51, 56, 1, 36, 57, 56]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode(enc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "70kPI6o-0unb",
        "outputId": "7d45e16a-f205-47c8-e8dd-419473e1e520"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Alice’s Adventures in Won'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode text\n",
        "encoded_text=encode(text)\n",
        "data = torch.tensor(encoded_text, dtype=torch.long)\n",
        "ndata=len(data)\n",
        "print(data[:1000])\n",
        "print()\n",
        "print(ndata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxxaVCPx0wVn",
        "outputId": "8ddb5338-cfe0-4d63-d7f4-57014be9996b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([14, 54, 51, 45, 47, 72, 61,  1, 14, 46, 64, 47, 56, 62, 63, 60, 47, 61,\n",
            "         1, 51, 56,  1, 36, 57, 56, 46, 47, 60, 54, 43, 56, 46,  0,  0, 44, 67,\n",
            "         1, 25, 47, 65, 51, 61,  1, 16, 43, 60, 60, 57, 54, 54,  0,  0, 33, 21,\n",
            "        18,  1, 26, 22, 25, 25, 18, 27, 27, 22, 34, 26,  1, 19, 34, 25, 16, 31,\n",
            "        34, 26,  1, 18, 17, 22, 33, 22, 28, 27,  1, 10,  8,  9,  0,  0, 16, 57,\n",
            "        56, 62, 47, 56, 62, 61,  0,  0,  1, 16, 21, 14, 29, 33, 18, 31,  1, 22,\n",
            "         8,  1,  1,  1,  1,  1, 17, 57, 65, 56,  1, 62, 50, 47,  1, 31, 43, 44,\n",
            "        44, 51, 62,  7, 21, 57, 54, 47,  0,  1, 16, 21, 14, 29, 33, 18, 31,  1,\n",
            "        22, 22,  8,  1,  1,  1,  1, 33, 50, 47,  1, 29, 57, 57, 54,  1, 57, 48,\n",
            "         1, 33, 47, 43, 60, 61,  0,  1, 16, 21, 14, 29, 33, 18, 31,  1, 22, 22,\n",
            "        22,  8,  1,  1,  1, 14,  1, 16, 43, 63, 45, 63, 61,  7, 31, 43, 45, 47,\n",
            "         1, 43, 56, 46,  1, 43,  1, 25, 57, 56, 49,  1, 33, 43, 54, 47,  0,  1,\n",
            "        16, 21, 14, 29, 33, 18, 31,  1, 22, 35,  8,  1,  1,  1,  1, 33, 50, 47,\n",
            "         1, 31, 43, 44, 44, 51, 62,  1, 32, 47, 56, 46, 61,  1, 51, 56,  1, 43,\n",
            "         1, 25, 51, 62, 62, 54, 47,  1, 15, 51, 54, 54,  0,  1, 16, 21, 14, 29,\n",
            "        33, 18, 31,  1, 35,  8,  1,  1,  1,  1,  1, 14, 46, 64, 51, 45, 47,  1,\n",
            "        48, 60, 57, 55,  1, 43,  1, 16, 43, 62, 47, 60, 58, 51, 54, 54, 43, 60,\n",
            "         0,  1, 16, 21, 14, 29, 33, 18, 31,  1, 35, 22,  8,  1,  1,  1,  1, 29,\n",
            "        51, 49,  1, 43, 56, 46,  1, 29, 47, 58, 58, 47, 60,  0,  1, 16, 21, 14,\n",
            "        29, 33, 18, 31,  1, 35, 22, 22,  8,  1,  1,  1, 14,  1, 26, 43, 46,  1,\n",
            "        33, 47, 43,  7, 29, 43, 60, 62, 67,  0,  1, 16, 21, 14, 29, 33, 18, 31,\n",
            "         1, 35, 22, 22, 22,  8,  1,  1, 33, 50, 47,  1, 30, 63, 47, 47, 56, 72,\n",
            "        61,  1, 16, 60, 57, 59, 63, 47, 62,  7, 20, 60, 57, 63, 56, 46,  0,  1,\n",
            "        16, 21, 14, 29, 33, 18, 31,  1, 22, 37,  8,  1,  1,  1,  1, 33, 50, 47,\n",
            "         1, 26, 57, 45, 53,  1, 33, 63, 60, 62, 54, 47, 72, 61,  1, 32, 62, 57,\n",
            "        60, 67,  0,  1, 16, 21, 14, 29, 33, 18, 31,  1, 37,  8,  1,  1,  1,  1,\n",
            "         1, 33, 50, 47,  1, 25, 57, 44, 61, 62, 47, 60,  1, 30, 63, 43, 46, 60,\n",
            "        51, 54, 54, 47,  0,  1, 16, 21, 14, 29, 33, 18, 31,  1, 37, 22,  8,  1,\n",
            "         1,  1,  1, 36, 50, 57,  1, 32, 62, 57, 54, 47,  1, 62, 50, 47,  1, 33,\n",
            "        43, 60, 62, 61, 13,  0,  1, 16, 21, 14, 29, 33, 18, 31,  1, 37, 22, 22,\n",
            "         8,  1,  1,  1, 14, 54, 51, 45, 47, 72, 61,  1, 18, 64, 51, 46, 47, 56,\n",
            "        45, 47,  0,  0,  0,  0,  0, 16, 21, 14, 29, 33, 18, 31,  1, 22,  8,  0,\n",
            "        17, 57, 65, 56,  1, 62, 50, 47,  1, 31, 43, 44, 44, 51, 62,  7, 21, 57,\n",
            "        54, 47,  0,  0,  0, 14, 54, 51, 45, 47,  1, 65, 43, 61,  1, 44, 47, 49,\n",
            "        51, 56, 56, 51, 56, 49,  1, 62, 57,  1, 49, 47, 62,  1, 64, 47, 60, 67,\n",
            "         1, 62, 51, 60, 47, 46,  1, 57, 48,  1, 61, 51, 62, 62, 51, 56, 49,  1,\n",
            "        44, 67,  1, 50, 47, 60,  1, 61, 51, 61, 62, 47, 60,  1, 57, 56,  1, 62,\n",
            "        50, 47,  0, 44, 43, 56, 53,  6,  1, 43, 56, 46,  1, 57, 48,  1, 50, 43,\n",
            "        64, 51, 56, 49,  1, 56, 57, 62, 50, 51, 56, 49,  1, 62, 57,  1, 46, 57,\n",
            "        11,  1, 57, 56, 45, 47,  1, 57, 60,  1, 62, 65, 51, 45, 47,  1, 61, 50,\n",
            "        47,  1, 50, 43, 46,  1, 58, 47, 47, 58, 47, 46,  1, 51, 56, 62, 57,  0,\n",
            "        62, 50, 47,  1, 44, 57, 57, 53,  1, 50, 47, 60,  1, 61, 51, 61, 62, 47,\n",
            "        60,  1, 65, 43, 61,  1, 60, 47, 43, 46, 51, 56, 49,  6,  1, 44, 63, 62,\n",
            "         1, 51, 62,  1, 50, 43, 46,  1, 56, 57,  1, 58, 51, 45, 62, 63, 60, 47,\n",
            "        61,  1, 57, 60,  0, 45, 57, 56, 64, 47, 60, 61, 43, 62, 51, 57, 56, 61,\n",
            "         1, 51, 56,  1, 51, 62,  6,  1, 73, 43, 56, 46,  1, 65, 50, 43, 62,  1,\n",
            "        51, 61,  1, 62, 50, 47,  1, 63, 61, 47,  1, 57, 48,  1, 43,  1, 44, 57,\n",
            "        57, 53,  6, 74,  1, 62, 50, 57, 63, 49, 50, 62,  1, 14, 54, 51, 45, 47,\n",
            "         0, 73, 65, 51, 62, 50, 57, 63, 62,  1, 58, 51, 45, 62, 63, 60, 47, 61,\n",
            "         1, 57, 60,  1, 45, 57, 56, 64, 47, 60, 61, 43, 62, 51, 57, 56, 61, 13,\n",
            "        74,  0,  0, 32, 57,  1, 61, 50, 47,  1, 65, 43, 61,  1, 45, 57, 56, 61,\n",
            "        51, 46, 47, 60, 51, 56, 49,  1, 51, 56,  1, 50, 47, 60,  1, 57, 65, 56,\n",
            "         1, 55, 51, 56, 46,  1,  3, 43, 61,  1, 65, 47, 54, 54,  1, 43, 61,  1,\n",
            "        61, 50, 47,  1, 45, 57, 63, 54, 46,  6,  1, 48, 57, 60,  1, 62, 50, 47,\n",
            "         0, 50, 57, 62,  1, 46, 43, 67,  1, 55, 43, 46, 47,  1, 50, 47, 60,  1,\n",
            "        48, 47, 47, 54,  1, 64, 47, 60, 67,  1])\n",
            "\n",
            "144584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test split\n",
        "ntrain = int(0.7*ndata) # first 70% will be train, rest val\n",
        "train_data = data[:ntrain]\n",
        "val_data = data[ntrain:]\n",
        "nval=len(val_data)\n",
        "\n",
        "print(ntrain)\n",
        "print(nval)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaSNpocN0ylb",
        "outputId": "7176d700-0c4c-456b-cc00-ea74ecd163c4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101208\n",
            "43376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix]) #characters\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) #next characters\n",
        "    x,y = x.to(device), y.to(device)\n",
        "    return x,y"
      ],
      "metadata": {
        "id": "7VHYyUby02Kv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = get_batch('train')\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOT4-XKL03bX",
        "outputId": "7609c90d-6e8f-4d95-db2d-3aea12b8c9cf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 256])\n",
            "torch.Size([128, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x[0])\n",
        "print()\n",
        "print(x[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGZ3kQOe04JX",
        "outputId": "bcff2ed3-f68b-4a6d-e6a0-7d8ce361fb23"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([56, 46,  1, 29, 47, 58, 58, 47, 60,  0,  0,  0, 19, 57, 60,  1, 43,  1,\n",
            "        55, 51, 56, 63, 62, 47,  1, 57, 60,  1, 62, 65, 57,  1, 61, 50, 47,  1,\n",
            "        61, 62, 57, 57, 46,  1, 54, 57, 57, 53, 51, 56, 49,  1, 43, 62,  1, 62,\n",
            "        50, 47,  1, 50, 57, 63, 61, 47,  6,  1, 43, 56, 46,  1, 65, 57, 56, 46,\n",
            "        47, 60, 51, 56, 49,  1, 65, 50, 43, 62,  0, 62, 57,  1, 46, 57,  1, 56,\n",
            "        47, 66, 62,  6,  1, 65, 50, 47, 56,  1, 61, 63, 46, 46, 47, 56, 54, 67,\n",
            "         1, 43,  1, 48, 57, 57, 62, 55, 43, 56,  1, 51, 56,  1, 54, 51, 64, 47,\n",
            "        60, 67,  1, 45, 43, 55, 47,  1, 60, 63, 56, 56, 51, 56, 49,  1, 57, 63,\n",
            "        62,  1, 57, 48,  1, 62, 50, 47,  0, 65, 57, 57, 46, 70,  3, 61, 50, 47,\n",
            "         1, 45, 57, 56, 61, 51, 46, 47, 60, 47, 46,  1, 50, 51, 55,  1, 62, 57,\n",
            "         1, 44, 47,  1, 43,  1, 48, 57, 57, 62, 55, 43, 56,  1, 44, 47, 45, 43,\n",
            "        63, 61, 47,  1, 50, 47,  1, 65, 43, 61,  1, 51, 56,  1, 54, 51, 64, 47,\n",
            "        60, 67, 11,  0, 57, 62, 50, 47, 60, 65, 51, 61, 47,  6,  1, 52, 63, 46,\n",
            "        49, 51, 56, 49,  1, 44, 67,  1, 50, 51, 61,  1, 48, 43, 45, 47,  1, 57,\n",
            "        56, 54, 67,  6], device='cuda:0')\n",
            "\n",
            "torch.Size([256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXUdu9Q205q-",
        "outputId": "59878451-b932-4b1a-e690-e574a717bf6a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([46,  1, 29, 47, 58, 58, 47, 60,  0,  0,  0, 19, 57, 60,  1, 43,  1, 55,\n",
              "        51, 56, 63, 62, 47,  1, 57, 60,  1, 62, 65, 57,  1, 61, 50, 47,  1, 61,\n",
              "        62, 57, 57, 46,  1, 54, 57, 57, 53, 51, 56, 49,  1, 43, 62,  1, 62, 50,\n",
              "        47,  1, 50, 57, 63, 61, 47,  6,  1, 43, 56, 46,  1, 65, 57, 56, 46, 47,\n",
              "        60, 51, 56, 49,  1, 65, 50, 43, 62,  0, 62, 57,  1, 46, 57,  1, 56, 47,\n",
              "        66, 62,  6,  1, 65, 50, 47, 56,  1, 61, 63, 46, 46, 47, 56, 54, 67,  1,\n",
              "        43,  1, 48, 57, 57, 62, 55, 43, 56,  1, 51, 56,  1, 54, 51, 64, 47, 60,\n",
              "        67,  1, 45, 43, 55, 47,  1, 60, 63, 56, 56, 51, 56, 49,  1, 57, 63, 62,\n",
              "         1, 57, 48,  1, 62, 50, 47,  0, 65, 57, 57, 46, 70,  3, 61, 50, 47,  1,\n",
              "        45, 57, 56, 61, 51, 46, 47, 60, 47, 46,  1, 50, 51, 55,  1, 62, 57,  1,\n",
              "        44, 47,  1, 43,  1, 48, 57, 57, 62, 55, 43, 56,  1, 44, 47, 45, 43, 63,\n",
              "        61, 47,  1, 50, 47,  1, 65, 43, 61,  1, 51, 56,  1, 54, 51, 64, 47, 60,\n",
              "        67, 11,  0, 57, 62, 50, 47, 60, 65, 51, 61, 47,  6,  1, 52, 63, 46, 49,\n",
              "        51, 56, 49,  1, 44, 67,  1, 50, 51, 61,  1, 48, 43, 45, 47,  1, 57, 56,\n",
              "        54, 67,  6,  1], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Goal of nanoGPT: given a sequence of characters, predict the next character\n",
        "#one block of training data gives the following training information:\n",
        "K=10\n",
        "for k in range(K):\n",
        "  print(str(k+1)+': previous characters:'+str(x[0][:k+1].tolist())+' -> next character: '+str(y[0][k].tolist()))\n",
        "print()\n",
        "print(str(block_size)+': previous characters:'+str(x[0].tolist())+' -> next character: '+str(y[0][-1].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYZZiBDq09A2",
        "outputId": "0e06e40b-981e-4fb1-c1f8-3217e572966b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: previous characters:[56] -> next character: 46\n",
            "2: previous characters:[56, 46] -> next character: 1\n",
            "3: previous characters:[56, 46, 1] -> next character: 29\n",
            "4: previous characters:[56, 46, 1, 29] -> next character: 47\n",
            "5: previous characters:[56, 46, 1, 29, 47] -> next character: 58\n",
            "6: previous characters:[56, 46, 1, 29, 47, 58] -> next character: 58\n",
            "7: previous characters:[56, 46, 1, 29, 47, 58, 58] -> next character: 47\n",
            "8: previous characters:[56, 46, 1, 29, 47, 58, 58, 47] -> next character: 60\n",
            "9: previous characters:[56, 46, 1, 29, 47, 58, 58, 47, 60] -> next character: 0\n",
            "10: previous characters:[56, 46, 1, 29, 47, 58, 58, 47, 60, 0] -> next character: 0\n",
            "\n",
            "256: previous characters:[56, 46, 1, 29, 47, 58, 58, 47, 60, 0, 0, 0, 19, 57, 60, 1, 43, 1, 55, 51, 56, 63, 62, 47, 1, 57, 60, 1, 62, 65, 57, 1, 61, 50, 47, 1, 61, 62, 57, 57, 46, 1, 54, 57, 57, 53, 51, 56, 49, 1, 43, 62, 1, 62, 50, 47, 1, 50, 57, 63, 61, 47, 6, 1, 43, 56, 46, 1, 65, 57, 56, 46, 47, 60, 51, 56, 49, 1, 65, 50, 43, 62, 0, 62, 57, 1, 46, 57, 1, 56, 47, 66, 62, 6, 1, 65, 50, 47, 56, 1, 61, 63, 46, 46, 47, 56, 54, 67, 1, 43, 1, 48, 57, 57, 62, 55, 43, 56, 1, 51, 56, 1, 54, 51, 64, 47, 60, 67, 1, 45, 43, 55, 47, 1, 60, 63, 56, 56, 51, 56, 49, 1, 57, 63, 62, 1, 57, 48, 1, 62, 50, 47, 0, 65, 57, 57, 46, 70, 3, 61, 50, 47, 1, 45, 57, 56, 61, 51, 46, 47, 60, 47, 46, 1, 50, 51, 55, 1, 62, 57, 1, 44, 47, 1, 43, 1, 48, 57, 57, 62, 55, 43, 56, 1, 44, 47, 45, 43, 63, 61, 47, 1, 50, 47, 1, 65, 43, 61, 1, 51, 56, 1, 54, 51, 64, 47, 60, 67, 11, 0, 57, 62, 50, 47, 60, 65, 51, 61, 47, 6, 1, 52, 63, 46, 49, 51, 56, 49, 1, 44, 67, 1, 50, 51, 61, 1, 48, 43, 45, 47, 1, 57, 56, 54, 67, 6] -> next character: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT model train"
      ],
      "metadata": {
        "id": "yh1bpGde1C4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nanoGPT-model\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size): # head_size = n_embd//n_head\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size) # K_Weights [head_size,n_embd] + K_bias [head_size]\n",
        "        self.query = nn.Linear(n_embd, head_size) # Q_Weights [head_size,n_embd]  +Q_bias [head_size]\n",
        "        self.value = nn.Linear(n_embd, head_size) # V_Weights [head_size,n_embd] + V_bias [head_size]\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) #mask, [block_size,block_size], tril = lower triangular\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size [B,T,n_embd]\n",
        "        # output of size [B,T,head_size]\n",
        "        B,T,C = x.shape\n",
        "        K_vectors = self.key(x)   # K_vectors=x@K_weights.T+K_bias, [B,T,head_size]\n",
        "        Q_vectors = self.query(x) # Q_vectors=x@Q_weights.T+Q_bias, [B,T,head_size]\n",
        "        # compute attention scores (\"affinities\")\n",
        "        Attention_Matrix = Q_vectors @ K_vectors.transpose(-2,-1) * K_vectors.shape[-1]**-0.5 # [B,T,T]\n",
        "        Attention_Matrix = Attention_Matrix.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # [B,T,T], upper diagonal elements to -infinity\n",
        "        Attn_Matrix_Softmax = F.softmax(Attention_Matrix, dim=-1) # [B,T,T], exp(-inf) = 0 -> upper diagonal elements = 0\n",
        "        Attn_Matrix_Softmax = self.dropout(Attn_Matrix_Softmax)\n",
        "        # perform the weighted aggregation of the values\n",
        "        V_vectors = self.value(x) # V_vectors=x@V_weights.T+V_bias, [B,T,head_size]\n",
        "        V_output = Attn_Matrix_Softmax@V_vectors #  [B,T,head_size]\n",
        "        return V_output\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, n_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)]) #self-attention heads\n",
        "        self.proj = nn.Linear(head_size * n_heads, n_embd) # Projection_Weights [n_embd,n_embd] + Projection_Bias [n_embd]\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Stack_V_outputs = torch.cat([h(x) for h in self.heads], dim=-1) #stacked V_outputs, [B,T,n_embd]\n",
        "        Attention_Output = self.proj(Stack_V_outputs) # Attention_output = Stack_V_outputs@Projection_Weights.T+Projection_Bias, [B,T,n_embd]\n",
        "        Attention_Output = self.dropout(Attention_Output)\n",
        "        return Attention_Output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd), # MLP_weights [4*n_embd,n_embd] + MLP_Bias [4*n_embd]\n",
        "            nn.GELU(approximate='tanh'), # MLP_Activation, GaussianErrorLinearUnit\n",
        "            nn.Linear(4 * n_embd, n_embd), # MLP_Projection_Weights [n_embd,4*n_embd] + MLP_Projection_Bias [n_embd]\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # x = Layer_Norm, [B,T,n_embd]\n",
        "        return self.net(x) # MLP_Result\n",
        "        # MLP = x@MLP_weights.T + MLP_Bias, [B,T,4*n_embd]\n",
        "        # MLP_activation = GELU(MLP), [B,T,4*n_embd]\n",
        "        # MLP_result = MLP_activation@MLP_Projection_Weights.T+MLP_Projection_Bias, [B,T,n_embd]\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd) # layer normalization 1, weight gamma1 [n_embd], bias beta1 [n_embd]\n",
        "        self.ln2 = nn.LayerNorm(n_embd) # layer normalization 2, weight gamma2 [n_embd], bias beta2 [n_embd]\n",
        "\n",
        "    def forward(self, x): # x = [B,T,n_embd]\n",
        "        Layer_Norm1=self.ln1(x) # [B,T,n_embd]\n",
        "        Attention_Output=self.sa(Layer_Norm1) # [B,T,n_embd]\n",
        "        Attention_Residual=Attention_Output+x # [B,T,n_embd]\n",
        "        Layer_Norm2=self.ln2(Attention_Residual) # [B,T,n_embd]\n",
        "        MLP_Result=self.ffwd(Layer_Norm2) # [B,T,n_embd],\n",
        "        MLP_Residual=Attention_Residual+MLP_Result # [B,T,n_embd]\n",
        "        return MLP_Residual\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) # transformer blocks\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer normalization, weight gamma3 [n_embd], bias beta3 [n_embd]\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False) # LM_Head_Weights [vocab_size,n_embd]\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both [B,T] tensors of integers\n",
        "        Token_Embed = self.token_embedding_table(idx) # [B,T,n_embd]\n",
        "        Position_Embed = self.position_embedding_table(torch.arange(T, device=device)) # [T,n_embd]\n",
        "        Input_Embed = Token_Embed + Position_Embed # [B,T,n_embd]\n",
        "        Final_MLP_Residual = self.blocks(Input_Embed) # [B,T,n_embd]\n",
        "        Final_Layer_Norm = self.ln_f(Final_MLP_Residual) # [B,T,n_embd]\n",
        "        logits = self.lm_head(Final_Layer_Norm) # logits = Final_Layer_Norm@LM_Head_Weights.T, [B,T,vocab_size]\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B,T,vs = logits.shape #vs = vocab_size\n",
        "            logits = logits.view(B*T,vs) #[B,T,vs] -> [B*T,vs]\n",
        "            targets = targets.view(B*T) #[B,T] -> [B*T]\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_characters):\n",
        "      #idx is [B,T] array of indices in the current context\n",
        "      for _ in range(max_new_characters):\n",
        "          # crop idx to the last block_size tokens\n",
        "          idx_cond = idx[:, -block_size:]\n",
        "          # get the predictions\n",
        "          logits, loss = self(idx_cond) #Logits = [B,T,vocab_size]\n",
        "          # focus only on the last time step\n",
        "          logits = logits[:, -1, :] # becomes [B,vocab_size]\n",
        "          # apply softmax to get probabilities\n",
        "          probs = F.softmax(logits, dim=-1) # [B,vocab_size]\n",
        "          # sample from the distribution\n",
        "          idx_next = torch.multinomial(probs, num_samples=1) # [B,1]\n",
        "          # append sampled index to the running sequence\n",
        "          idx = torch.cat((idx, idx_next), dim=1) # [B,T+1]\n",
        "      return idx\n",
        "\n"
      ],
      "metadata": {
        "id": "xb5ODRQz0_IS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTLanguageModel().to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in model.parameters()), ' parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqti8suC1Jv2",
        "outputId": "37247df9-6a7d-431d-c5cf-2028ead43876"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "845312  parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "  if param.requires_grad: #trainable parameters\n",
        "    print(name)\n",
        "    print(param.data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7GX0LoY1LM2",
        "outputId": "1201cdd9-7e7a-4cf2-c3ef-bd4cd60077a5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token_embedding_table.weight\n",
            "torch.Size([75, 128])\n",
            "position_embedding_table.weight\n",
            "torch.Size([256, 128])\n",
            "blocks.0.sa.heads.0.key.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.0.sa.heads.0.key.bias\n",
            "torch.Size([32])\n",
            "blocks.0.sa.heads.0.query.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.0.sa.heads.0.query.bias\n",
            "torch.Size([32])\n",
            "blocks.0.sa.heads.0.value.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.0.sa.heads.0.value.bias\n",
            "torch.Size([32])\n",
            "blocks.0.sa.heads.1.key.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.0.sa.heads.1.key.bias\n",
            "torch.Size([32])\n",
            "blocks.0.sa.heads.1.query.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.0.sa.heads.1.query.bias\n",
            "torch.Size([32])\n",
            "blocks.0.sa.heads.1.value.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.0.sa.heads.1.value.bias\n",
            "torch.Size([32])\n",
            "blocks.0.sa.heads.2.key.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.0.sa.heads.2.key.bias\n",
            "torch.Size([32])\n",
            "blocks.0.sa.heads.2.query.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.0.sa.heads.2.query.bias\n",
            "torch.Size([32])\n",
            "blocks.0.sa.heads.2.value.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.0.sa.heads.2.value.bias\n",
            "torch.Size([32])\n",
            "blocks.0.sa.heads.3.key.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.0.sa.heads.3.key.bias\n",
            "torch.Size([32])\n",
            "blocks.0.sa.heads.3.query.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.0.sa.heads.3.query.bias\n",
            "torch.Size([32])\n",
            "blocks.0.sa.heads.3.value.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.0.sa.heads.3.value.bias\n",
            "torch.Size([32])\n",
            "blocks.0.sa.proj.weight\n",
            "torch.Size([128, 128])\n",
            "blocks.0.sa.proj.bias\n",
            "torch.Size([128])\n",
            "blocks.0.ffwd.net.0.weight\n",
            "torch.Size([512, 128])\n",
            "blocks.0.ffwd.net.0.bias\n",
            "torch.Size([512])\n",
            "blocks.0.ffwd.net.2.weight\n",
            "torch.Size([128, 512])\n",
            "blocks.0.ffwd.net.2.bias\n",
            "torch.Size([128])\n",
            "blocks.0.ln1.weight\n",
            "torch.Size([128])\n",
            "blocks.0.ln1.bias\n",
            "torch.Size([128])\n",
            "blocks.0.ln2.weight\n",
            "torch.Size([128])\n",
            "blocks.0.ln2.bias\n",
            "torch.Size([128])\n",
            "blocks.1.sa.heads.0.key.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.1.sa.heads.0.key.bias\n",
            "torch.Size([32])\n",
            "blocks.1.sa.heads.0.query.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.1.sa.heads.0.query.bias\n",
            "torch.Size([32])\n",
            "blocks.1.sa.heads.0.value.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.1.sa.heads.0.value.bias\n",
            "torch.Size([32])\n",
            "blocks.1.sa.heads.1.key.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.1.sa.heads.1.key.bias\n",
            "torch.Size([32])\n",
            "blocks.1.sa.heads.1.query.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.1.sa.heads.1.query.bias\n",
            "torch.Size([32])\n",
            "blocks.1.sa.heads.1.value.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.1.sa.heads.1.value.bias\n",
            "torch.Size([32])\n",
            "blocks.1.sa.heads.2.key.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.1.sa.heads.2.key.bias\n",
            "torch.Size([32])\n",
            "blocks.1.sa.heads.2.query.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.1.sa.heads.2.query.bias\n",
            "torch.Size([32])\n",
            "blocks.1.sa.heads.2.value.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.1.sa.heads.2.value.bias\n",
            "torch.Size([32])\n",
            "blocks.1.sa.heads.3.key.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.1.sa.heads.3.key.bias\n",
            "torch.Size([32])\n",
            "blocks.1.sa.heads.3.query.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.1.sa.heads.3.query.bias\n",
            "torch.Size([32])\n",
            "blocks.1.sa.heads.3.value.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.1.sa.heads.3.value.bias\n",
            "torch.Size([32])\n",
            "blocks.1.sa.proj.weight\n",
            "torch.Size([128, 128])\n",
            "blocks.1.sa.proj.bias\n",
            "torch.Size([128])\n",
            "blocks.1.ffwd.net.0.weight\n",
            "torch.Size([512, 128])\n",
            "blocks.1.ffwd.net.0.bias\n",
            "torch.Size([512])\n",
            "blocks.1.ffwd.net.2.weight\n",
            "torch.Size([128, 512])\n",
            "blocks.1.ffwd.net.2.bias\n",
            "torch.Size([128])\n",
            "blocks.1.ln1.weight\n",
            "torch.Size([128])\n",
            "blocks.1.ln1.bias\n",
            "torch.Size([128])\n",
            "blocks.1.ln2.weight\n",
            "torch.Size([128])\n",
            "blocks.1.ln2.bias\n",
            "torch.Size([128])\n",
            "blocks.2.sa.heads.0.key.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.2.sa.heads.0.key.bias\n",
            "torch.Size([32])\n",
            "blocks.2.sa.heads.0.query.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.2.sa.heads.0.query.bias\n",
            "torch.Size([32])\n",
            "blocks.2.sa.heads.0.value.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.2.sa.heads.0.value.bias\n",
            "torch.Size([32])\n",
            "blocks.2.sa.heads.1.key.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.2.sa.heads.1.key.bias\n",
            "torch.Size([32])\n",
            "blocks.2.sa.heads.1.query.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.2.sa.heads.1.query.bias\n",
            "torch.Size([32])\n",
            "blocks.2.sa.heads.1.value.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.2.sa.heads.1.value.bias\n",
            "torch.Size([32])\n",
            "blocks.2.sa.heads.2.key.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.2.sa.heads.2.key.bias\n",
            "torch.Size([32])\n",
            "blocks.2.sa.heads.2.query.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.2.sa.heads.2.query.bias\n",
            "torch.Size([32])\n",
            "blocks.2.sa.heads.2.value.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.2.sa.heads.2.value.bias\n",
            "torch.Size([32])\n",
            "blocks.2.sa.heads.3.key.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.2.sa.heads.3.key.bias\n",
            "torch.Size([32])\n",
            "blocks.2.sa.heads.3.query.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.2.sa.heads.3.query.bias\n",
            "torch.Size([32])\n",
            "blocks.2.sa.heads.3.value.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.2.sa.heads.3.value.bias\n",
            "torch.Size([32])\n",
            "blocks.2.sa.proj.weight\n",
            "torch.Size([128, 128])\n",
            "blocks.2.sa.proj.bias\n",
            "torch.Size([128])\n",
            "blocks.2.ffwd.net.0.weight\n",
            "torch.Size([512, 128])\n",
            "blocks.2.ffwd.net.0.bias\n",
            "torch.Size([512])\n",
            "blocks.2.ffwd.net.2.weight\n",
            "torch.Size([128, 512])\n",
            "blocks.2.ffwd.net.2.bias\n",
            "torch.Size([128])\n",
            "blocks.2.ln1.weight\n",
            "torch.Size([128])\n",
            "blocks.2.ln1.bias\n",
            "torch.Size([128])\n",
            "blocks.2.ln2.weight\n",
            "torch.Size([128])\n",
            "blocks.2.ln2.bias\n",
            "torch.Size([128])\n",
            "blocks.3.sa.heads.0.key.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.3.sa.heads.0.key.bias\n",
            "torch.Size([32])\n",
            "blocks.3.sa.heads.0.query.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.3.sa.heads.0.query.bias\n",
            "torch.Size([32])\n",
            "blocks.3.sa.heads.0.value.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.3.sa.heads.0.value.bias\n",
            "torch.Size([32])\n",
            "blocks.3.sa.heads.1.key.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.3.sa.heads.1.key.bias\n",
            "torch.Size([32])\n",
            "blocks.3.sa.heads.1.query.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.3.sa.heads.1.query.bias\n",
            "torch.Size([32])\n",
            "blocks.3.sa.heads.1.value.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.3.sa.heads.1.value.bias\n",
            "torch.Size([32])\n",
            "blocks.3.sa.heads.2.key.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.3.sa.heads.2.key.bias\n",
            "torch.Size([32])\n",
            "blocks.3.sa.heads.2.query.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.3.sa.heads.2.query.bias\n",
            "torch.Size([32])\n",
            "blocks.3.sa.heads.2.value.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.3.sa.heads.2.value.bias\n",
            "torch.Size([32])\n",
            "blocks.3.sa.heads.3.key.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.3.sa.heads.3.key.bias\n",
            "torch.Size([32])\n",
            "blocks.3.sa.heads.3.query.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.3.sa.heads.3.query.bias\n",
            "torch.Size([32])\n",
            "blocks.3.sa.heads.3.value.weight\n",
            "torch.Size([32, 128])\n",
            "blocks.3.sa.heads.3.value.bias\n",
            "torch.Size([32])\n",
            "blocks.3.sa.proj.weight\n",
            "torch.Size([128, 128])\n",
            "blocks.3.sa.proj.bias\n",
            "torch.Size([128])\n",
            "blocks.3.ffwd.net.0.weight\n",
            "torch.Size([512, 128])\n",
            "blocks.3.ffwd.net.0.bias\n",
            "torch.Size([512])\n",
            "blocks.3.ffwd.net.2.weight\n",
            "torch.Size([128, 512])\n",
            "blocks.3.ffwd.net.2.bias\n",
            "torch.Size([128])\n",
            "blocks.3.ln1.weight\n",
            "torch.Size([128])\n",
            "blocks.3.ln1.bias\n",
            "torch.Size([128])\n",
            "blocks.3.ln2.weight\n",
            "torch.Size([128])\n",
            "blocks.3.ln2.bias\n",
            "torch.Size([128])\n",
            "ln_f.weight\n",
            "torch.Size([128])\n",
            "ln_f.bias\n",
            "torch.Size([128])\n",
            "lm_head.weight\n",
            "torch.Size([75, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for training to calculate losses and accuracies\n",
        "@torch.no_grad() #no backpropagation used\n",
        "def estimate_loss():\n",
        "    losses = {}\n",
        "    accuracies={}\n",
        "    model.eval() #model to evaluation (prediction) phase\n",
        "    for split in ['train', 'val']:\n",
        "        loss_split = torch.zeros(eval_iters)\n",
        "        correct_split=torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            loss_split[k] = loss.item()\n",
        "            logits=logits.view(batch_size,block_size, vocab_size) #[batch_size*block_size,vocab_size] -> [batch_size,block_size,vocab_size]\n",
        "            predictions=torch.argmax(logits,axis=-1) #predicted next characters\n",
        "            correct_split[k]=(predictions==Y).sum().item() #number of correct predictions\n",
        "        losses[split] = loss_split.mean()\n",
        "        accuracies[split]=correct_split.sum()/(eval_iters*batch_size*block_size)\n",
        "    model.train() #model to training phase\n",
        "    return losses, accuracies"
      ],
      "metadata": {
        "id": "absTqMLi1MXQ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNpw8cLg1O4p",
        "outputId": "7bed922d-d7b3-4cc1-c0ba-976b278dd163"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for training create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "oEHsM2V41TOC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model, use GPU\n",
        "\n",
        "best_val_acc=0\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses, accuracies  = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, train acc {accuracies['train']:.4f}, val loss {losses['val']:.4f}, val acc {accuracies['val']:.4f}\")\n",
        "        if accuracies['val']>best_val_acc:\n",
        "          best_val_acc=accuracies['val']\n",
        "          torch.save(model.state_dict(), '/content/drive/MyDrive/nanoGPT.pt') #save best weights\n",
        "          print('saved')\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb) #forward propagation\n",
        "    optimizer.zero_grad(set_to_none=True) #clear previous derivates\n",
        "    loss.backward() #backpropagation\n",
        "    optimizer.step() #update parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJjkQlay1WuG",
        "outputId": "b511a557-24c5-4206-b000-adf2bb45786f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3356, train acc 0.0111, val loss 4.3288, val acc 0.0124\n",
            "saved\n",
            "step 100: train loss 2.4897, train acc 0.2899, val loss 2.5232, val acc 0.2893\n",
            "saved\n",
            "step 200: train loss 2.4110, train acc 0.2965, val loss 2.4481, val acc 0.3001\n",
            "saved\n",
            "step 300: train loss 2.3202, train acc 0.3241, val loss 2.3773, val acc 0.3238\n",
            "saved\n",
            "step 400: train loss 2.0946, train acc 0.3883, val loss 2.1663, val acc 0.3804\n",
            "saved\n",
            "step 500: train loss 1.8237, train acc 0.4577, val loss 1.9303, val acc 0.4406\n",
            "saved\n",
            "step 600: train loss 1.6518, train acc 0.4981, val loss 1.8022, val acc 0.4761\n",
            "saved\n",
            "step 700: train loss 1.5258, train acc 0.5309, val loss 1.7201, val acc 0.5016\n",
            "saved\n",
            "step 800: train loss 1.4414, train acc 0.5517, val loss 1.6823, val acc 0.5131\n",
            "saved\n",
            "step 900: train loss 1.3506, train acc 0.5764, val loss 1.6387, val acc 0.5294\n",
            "saved\n",
            "step 1000: train loss 1.2830, train acc 0.5949, val loss 1.6043, val acc 0.5389\n",
            "saved\n",
            "step 1100: train loss 1.2194, train acc 0.6129, val loss 1.5777, val acc 0.5452\n",
            "saved\n",
            "step 1200: train loss 1.1656, train acc 0.6289, val loss 1.5643, val acc 0.5536\n",
            "saved\n",
            "step 1300: train loss 1.1128, train acc 0.6440, val loss 1.5633, val acc 0.5571\n",
            "saved\n",
            "step 1400: train loss 1.0742, train acc 0.6553, val loss 1.5645, val acc 0.5590\n",
            "saved\n",
            "step 1500: train loss 1.0315, train acc 0.6670, val loss 1.5647, val acc 0.5655\n",
            "saved\n",
            "step 1600: train loss 0.9882, train acc 0.6801, val loss 1.5597, val acc 0.5680\n",
            "saved\n",
            "step 1700: train loss 0.9535, train acc 0.6909, val loss 1.5632, val acc 0.5691\n",
            "saved\n",
            "step 1800: train loss 0.9215, train acc 0.6988, val loss 1.5791, val acc 0.5693\n",
            "saved\n",
            "step 1900: train loss 0.8886, train acc 0.7093, val loss 1.5727, val acc 0.5693\n",
            "saved\n",
            "step 2000: train loss 0.8579, train acc 0.7193, val loss 1.5883, val acc 0.5731\n",
            "saved\n",
            "step 2100: train loss 0.8311, train acc 0.7277, val loss 1.5987, val acc 0.5732\n",
            "saved\n",
            "step 2200: train loss 0.8059, train acc 0.7364, val loss 1.6061, val acc 0.5723\n",
            "step 2300: train loss 0.7807, train acc 0.7437, val loss 1.6137, val acc 0.5736\n",
            "saved\n",
            "step 2400: train loss 0.7559, train acc 0.7523, val loss 1.6176, val acc 0.5751\n",
            "saved\n",
            "step 2500: train loss 0.7412, train acc 0.7555, val loss 1.6307, val acc 0.5756\n",
            "saved\n",
            "step 2600: train loss 0.7139, train acc 0.7648, val loss 1.6558, val acc 0.5745\n",
            "step 2700: train loss 0.6919, train acc 0.7734, val loss 1.6499, val acc 0.5751\n",
            "step 2800: train loss 0.6719, train acc 0.7798, val loss 1.6601, val acc 0.5756\n",
            "step 2900: train loss 0.6489, train acc 0.7886, val loss 1.6889, val acc 0.5726\n",
            "step 3000: train loss 0.6317, train acc 0.7939, val loss 1.6945, val acc 0.5735\n",
            "step 3100: train loss 0.6199, train acc 0.7983, val loss 1.7160, val acc 0.5739\n",
            "step 3200: train loss 0.5968, train acc 0.8062, val loss 1.7303, val acc 0.5729\n",
            "step 3300: train loss 0.5861, train acc 0.8094, val loss 1.7399, val acc 0.5736\n",
            "step 3400: train loss 0.5692, train acc 0.8144, val loss 1.7478, val acc 0.5732\n",
            "step 3500: train loss 0.5573, train acc 0.8198, val loss 1.7552, val acc 0.5743\n",
            "step 3600: train loss 0.5355, train acc 0.8273, val loss 1.7690, val acc 0.5730\n",
            "step 3700: train loss 0.5291, train acc 0.8285, val loss 1.7866, val acc 0.5731\n",
            "step 3800: train loss 0.5186, train acc 0.8340, val loss 1.7893, val acc 0.5719\n",
            "step 3900: train loss 0.5036, train acc 0.8385, val loss 1.8019, val acc 0.5715\n",
            "step 4000: train loss 0.4912, train acc 0.8434, val loss 1.8014, val acc 0.5722\n",
            "step 4100: train loss 0.4782, train acc 0.8467, val loss 1.8402, val acc 0.5710\n",
            "step 4200: train loss 0.4665, train acc 0.8514, val loss 1.8376, val acc 0.5694\n",
            "step 4300: train loss 0.4573, train acc 0.8557, val loss 1.8346, val acc 0.5711\n",
            "step 4400: train loss 0.4474, train acc 0.8573, val loss 1.8651, val acc 0.5728\n",
            "step 4500: train loss 0.4397, train acc 0.8605, val loss 1.8759, val acc 0.5708\n",
            "step 4600: train loss 0.4261, train acc 0.8669, val loss 1.8667, val acc 0.5691\n",
            "step 4700: train loss 0.4181, train acc 0.8681, val loss 1.8863, val acc 0.5714\n",
            "step 4800: train loss 0.4099, train acc 0.8716, val loss 1.8919, val acc 0.5709\n",
            "step 4900: train loss 0.4015, train acc 0.8732, val loss 1.9140, val acc 0.5706\n",
            "step 4999: train loss 0.3911, train acc 0.8780, val loss 1.9205, val acc 0.5700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define GPTLanguageModel and load saved weights\n",
        "\n",
        "#using CPU\n",
        "#model.load_state_dict(torch.load('/content/drive/MyDrive/nanoGPT.pt', weights_only=True, map_location=torch.device('cpu')))\n",
        "\n",
        "#using GPU\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/nanoGPT.pt', weights_only=True))\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-RPaaRBzgml",
        "outputId": "740aaa41-c5d8-44de-de0a-1dc2de56da2c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTLanguageModel(\n",
              "  (token_embedding_table): Embedding(75, 128)\n",
              "  (position_embedding_table): Embedding(256, 128)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): Linear(in_features=128, out_features=32, bias=True)\n",
              "            (query): Linear(in_features=128, out_features=32, bias=True)\n",
              "            (value): Linear(in_features=128, out_features=32, bias=True)\n",
              "            (dropout): Dropout(p=0.3, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "          (1): GELU(approximate='tanh')\n",
              "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (3): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): Linear(in_features=128, out_features=32, bias=True)\n",
              "            (query): Linear(in_features=128, out_features=32, bias=True)\n",
              "            (value): Linear(in_features=128, out_features=32, bias=True)\n",
              "            (dropout): Dropout(p=0.3, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "          (1): GELU(approximate='tanh')\n",
              "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (3): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): Linear(in_features=128, out_features=32, bias=True)\n",
              "            (query): Linear(in_features=128, out_features=32, bias=True)\n",
              "            (value): Linear(in_features=128, out_features=32, bias=True)\n",
              "            (dropout): Dropout(p=0.3, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "          (1): GELU(approximate='tanh')\n",
              "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (3): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): Linear(in_features=128, out_features=32, bias=True)\n",
              "            (query): Linear(in_features=128, out_features=32, bias=True)\n",
              "            (value): Linear(in_features=128, out_features=32, bias=True)\n",
              "            (dropout): Dropout(p=0.3, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "          (1): GELU(approximate='tanh')\n",
              "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (3): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=128, out_features=75, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "save_path = \"/content/drive/MyDrive/nanoGPT_parameters/\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Save trainable parameters\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:  # Trainable parameters\n",
        "        np.savetxt(os.path.join(save_path, f\"{name}.txt\"), param.data.cpu().detach().numpy())"
      ],
      "metadata": {
        "id": "lgksNIL11n6L"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate new characters from the model\n",
        "#context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "context = torch.tensor([encode('Alice to her eyes')], dtype=torch.long, device=device)\n",
        "max_new_characters=1000\n",
        "new_characters=model.generate(context, max_new_characters)\n",
        "print(decode(new_characters[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wzukLRf18ws",
        "outputId": "d59687a1-74f0-481d-f246-85624d46e5de"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alice to her eyes to, for I dare\n",
            "old dry know, and white Rabbit was to go notice look and the looked at\n",
            "it it. “Dinah!” Alice only replied very much Mouse, “but I know oh, and\n",
            "yet, or Majesty place you know! A back o’clain the moment, and Esquite Rabbit of Hearts: now\n",
            "She and felt tired have great got up know on flown to little golden, and\n",
            "neighten the door, who was only\n",
            "a confusion the Queen.”\n",
            "\n",
            "“—who am I have can’t been your head?” shouted they! “I can doesn’t like think me\n",
            "things are! I’ll try inters about if I get in at first, I am I’m not I have go down\n",
            "from or here? The way are of the Party!”\n",
            "\n",
            "The Rabbit had begun in the Mouse. The Rabbit’s mouse in the way Alice quite\n",
            "again, cried to replied among—(she appeared had to be held down and sightener to her.\n",
            "\n",
            "“One mind, does!” Alice was to get get on the glass, and began thought to through the\n",
            "herself.\n",
            "\n",
            "“Why, it did then?” said the Cat. “I do readfully\n",
            "it is again,” replied Alice; and _very_ nearly forgot to the house should change to find a long more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# there also cat mouse rabit in the context of generated text"
      ],
      "metadata": {
        "id": "mxr3W1Td2WDF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}